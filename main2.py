import streamlit as st                     # ì›¹ ì¸í„°í˜ì´ìŠ¤ ì œì‘ì„ ìœ„í•œ Streamlit
import os                                   # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥ ì‚¬ìš©
import google.generativeai as genai        # Google Gemini AI APIë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥
from pdf_utils import extract_text_from_pdf # PDF ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê¸°ëŠ¥
import asyncio                              # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ asyncio ë¼ì´ë¸ŒëŸ¬ë¦¬
from concurrent.futures import ThreadPoolExecutor  # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor
from sklearn.feature_extraction.text import TfidfVectorizer  # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ê¸° ìœ„í•œ TF-IDF ë„êµ¬
from sklearn.metrics.pairwise import cosine_similarity       # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ í•¨ìˆ˜

# --- Streamlit í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ë¤í•‘ ì „ë¬¸ê°€ ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# --- ìœ ì €ë¡œë¶€í„° Gemini API Key ì…ë ¥ ë°›ê¸° ---
if 'gemini_api_key' not in st.session_state:
    st.session_state.gemini_api_key = ""

with st.sidebar.expander("ğŸ”‘ API Key ì„¤ì •", expanded=True):
    key_input = st.text_input(
        label="Google Gemini API Key ì…ë ¥",
        type="password",
        placeholder="ì—¬ê¸°ì— API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        value=st.session_state.gemini_api_key,
    )
    if key_input:
        st.session_state.gemini_api_key = key_input

if not st.session_state.gemini_api_key:
    st.sidebar.warning("ì±—ë´‡ì„ ì´ìš©í•˜ë ¤ë©´ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- Gemini API ì„¤ì • ---
genai.configure(api_key=st.session_state.gemini_api_key)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'law_data' not in st.session_state:
    st.session_state.law_data = {}
if 'selected_category' not in st.session_state:
    st.session_state.selected_category = None
if 'last_used_category' not in st.session_state:
    st.session_state.last_used_category = None
# ì„ë² ë”© ìºì‹±ìš© ìƒíƒœ
if 'embedding_data' not in st.session_state:
    st.session_state.embedding_data = {}
# ì¬ì‚¬ìš© ê°€ëŠ¥í•œ asyncio ì´ë²¤íŠ¸ ë£¨í”„
if 'event_loop' not in st.session_state:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    st.session_state.event_loop = loop

# --- ì¹´í…Œê³ ë¦¬ ì •ì˜ ---
LAW_CATEGORIES = {
    "ë¤í•‘ë°©ì§€ê´€ì„¸": {
        "ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™": "docs/ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™(ê¸°íšì¬ì •ë¶€ë ¹)(ì œ00940í˜¸)(20221025).pdf",
        "ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™": "docs/ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™(ê¸°íšì¬ì •ë¶€ë ¹)(ì œ00882í˜¸)(20220101) (1).pdf",
    },
    "ë¤í•‘íŒì •": {
        "ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ìµœì¢…íŒì •": "docs/ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸_ìµœì¢…íŒì •ì˜ê²°ì„œ.pdf",
        "ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ìµœì¢…íŒì •": "docs/ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸_ìµœì¢…íŒì •ì˜ê²°ì„œ.pdf",
    },
    "ê´€ë ¨ë²•ë ¹": {
        "ê´€ì„¸ë²•": "docs/ê´€ì„¸ë²•(ë²•ë¥ )(ì œ20608í˜¸)(20250401).pdf",
        "ê´€ì„¸ë²• ì‹œí–‰ë ¹": "docs/ê´€ì„¸ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹)(ì œ35363í˜¸)(20250722).pdf",
        "ê´€ì„¸ë²• ì‹œí–‰ê·œì¹™": "docs/ê´€ì„¸ë²• ì‹œí–‰ê·œì¹™(ê¸°íšì¬ì •ë¶€ë ¹)(ì œ01110í˜¸)(20250321).pdf",
        "ë¶ˆê³µì •ë¬´ì—­í–‰ìœ„ ì¡°ì‚¬ ë° ì‚°ì—…í”¼í•´êµ¬ì œì— ê´€í•œ ë²•ë¥ ": "docs/ë¶ˆê³µì •ë¬´ì—­í–‰ìœ„ ì¡°ì‚¬ ë° ì‚°ì—…í”¼í•´êµ¬ì œì— ê´€í•œ ë²•ë¥ (ë²•ë¥ )(ì œ20693í˜¸)(20250722).pdf",
    }
}

# ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì •ë³´
CATEGORY_KEYWORDS = {
    "ë¤í•‘ë°©ì§€ê´€ì„¸": ["ë¤í•‘ë°©ì§€ê´€ì„¸", "ë¤í•‘ë§ˆì§„", "ì •ìƒê°€ê²©", "ìˆ˜ì¶œê°€ê²©", "ë¤í•‘ë¥ ", "ë°˜ë¤í•‘ê´€ì„¸", "ë¤í•‘ë°©ì§€", "ë¤í•‘ë°©ì§€ì¡°ì¹˜"],
    "ë¤í•‘íŒì •": ["ìµœì¢…íŒì •", "ì˜ˆë¹„íŒì •", "ì‚°ì—…í”¼í•´", "ì‹¤ì§ˆì  í”¼í•´", "ì¸ê³¼ê´€ê³„", "êµ­ë‚´ì‚°ì—…", "ì¡°ì‚¬ëŒ€ìƒë¬¼í’ˆ", "ë¤í•‘ìˆ˜ì…"],
    "ê´€ë ¨ë²•ë ¹": ["ê´€ì„¸ë²•", "ì‹œí–‰ë ¹", "ì‹œí–‰ê·œì¹™", "ë¶ˆê³µì •ë¬´ì—­", "ì‚°ì—…í”¼í•´êµ¬ì œ", "ë¬´ì—­ìœ„ì›íšŒ", "ì¡°ì‚¬ì ˆì°¨", "ë¤í•‘ê·œì •"]
}

# PDF ë¡œë“œ ë° ì„ë² ë”© ìƒì„± í•¨ìˆ˜
@st.cache_data
def load_law_data(category=None):
    law_data = {}
    missing_files = []
    if category:
        pdf_files = LAW_CATEGORIES[category]
    else:
        pdf_files = {}
        for cat in LAW_CATEGORIES.values():
            pdf_files.update(cat)
    for law_name, pdf_path in pdf_files.items():
        if os.path.exists(pdf_path):
            text = extract_text_from_pdf(pdf_path)
            law_data[law_name] = text
            # ì„ë² ë”© ìƒì„± ë° ìºì‹±
            vec, mat, chunks = create_embeddings_for_text(text)
            st.session_state.embedding_data[law_name] = (vec, mat, chunks)
        else:
            missing_files.append(pdf_path)
    if missing_files:
        st.warning(f"ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
    return law_data

# ì„ë² ë”© ë° ì²­í¬ ìƒì„±
@st.cache_data
def create_embeddings_for_text(text, chunk_size=1000):
    chunks = []
    step = chunk_size // 2
    for i in range(0, len(text), step):
        segment = text[i:i+chunk_size]
        if len(segment) > 100:
            chunks.append(segment)
    vectorizer = TfidfVectorizer()
    matrix = vectorizer.fit_transform(chunks)
    return vectorizer, matrix, chunks

# ì¿¼ë¦¬ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
def search_relevant_chunks(query, vectorizer, tfidf_matrix, text_chunks, top_k=3, threshold=0.005):
    q_vec = vectorizer.transform([query])
    sims = cosine_similarity(q_vec, tfidf_matrix).flatten()
    indices = sims.argsort()[-top_k:][::-1]
    sel = [text_chunks[i] for i in indices if sims[i] > threshold]
    if not sel:
        sel = [text_chunks[i] for i in indices]
    return "\n\n".join(sel)

# Gemini ëª¨ë¸ ë°˜í™˜
def get_model():
    return genai.GenerativeModel('gemini-2.0-flash')

# ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•¨ìˆ˜ ì¶”ê°€
def classify_question_category(question):
    prompt = f"""
ë‹¹ì‹ ì€ ë¤í•‘ ê´€ë ¨ ë²•ë ¹ ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë²•ë ¹ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ëŠ” ì—…ë¬´ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì…ë‹ˆë‹¤:
"{question}"

ì•„ë˜ ë²•ë ¹ ì¹´í…Œê³ ë¦¬ ì¤‘ì—ì„œ ì´ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì¹´í…Œê³ ë¦¬ í•˜ë‚˜ë§Œ ì„ íƒí•´ì£¼ì„¸ìš”:

1. ë¤í•‘ë°©ì§€ê´€ì„¸: ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™, ë¤í•‘ë§ˆì§„, ì •ìƒê°€ê²©, ìˆ˜ì¶œê°€ê²© ë“± ê´€ë ¨
2. ë¤í•‘íŒì •: ìµœì¢…íŒì •ì˜ê²°ì„œ, ì˜ˆë¹„íŒì •, ì‚°ì—…í”¼í•´, ì‹¤ì§ˆì  í”¼í•´, ì¸ê³¼ê´€ê³„ ë“± ê´€ë ¨
3. ê´€ë ¨ë²•ë ¹: ê´€ì„¸ë²•, ë¶ˆê³µì •ë¬´ì—­í–‰ìœ„ ì¡°ì‚¬ ë° ì‚°ì—…í”¼í•´êµ¬ì œì— ê´€í•œ ë²•ë¥  ë“± ê¸°ë³¸ë²•ë ¹ ê´€ë ¨

ë°˜ë“œì‹œ ìœ„ì˜ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•˜ê³ , ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
"ì¹´í…Œê³ ë¦¬: [ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ëª…]"

ì˜ˆë¥¼ ë“¤ì–´, "ì¹´í…Œê³ ë¦¬: ë¤í•‘ë°©ì§€ê´€ì„¸"ì™€ ê°™ì´ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
    model = get_model()
    response = model.generate_content(prompt)
    # ì‘ë‹µì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
    response_text = response.text
    if "ì¹´í…Œê³ ë¦¬:" in response_text:
        category = response_text.split("ì¹´í…Œê³ ë¦¬:")[1].strip()
        # ì¹´í…Œê³ ë¦¬ëª…ë§Œ ì •í™•íˆ ì¶”ì¶œ
        for cat in LAW_CATEGORIES.keys():
            if cat in category:
                return cat
    # ë¶„ë¥˜ê°€ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
    return "ë¤í•‘ë°©ì§€ê´€ì„¸"  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ë¡œ ì„¤ì •

# ë²•ë ¹ë³„ ì—ì´ì „íŠ¸ ì‘ë‹µ (async)
async def get_law_agent_response_async(law_name, question, history):
    # ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if law_name not in st.session_state.embedding_data:
        text = st.session_state.law_data.get(law_name, "")
        vec, mat, chunks = create_embeddings_for_text(text)
        st.session_state.embedding_data[law_name] = (vec, mat, chunks)
    else:
        vec, mat, chunks = st.session_state.embedding_data[law_name]
    context = search_relevant_chunks(question, vec, mat, chunks)
    prompt = f"""
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ {law_name} ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë²•ë ¹ ë‚´ìš©ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ ë²•ë ¹ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
{context}

ì´ì „ ëŒ€í™”:
{history}

ì§ˆë¬¸: {question}

# ì‘ë‹µ ì§€ì¹¨
1. ì œê³µëœ ë²•ë ¹ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
2. ë‹µë³€ì— ì‚¬ìš©í•œ ëª¨ë“  ë²•ë ¹ ì¶œì²˜(ë²•ë ¹ëª…, ì¡°í•­)ë¥¼ ëª…í™•íˆ ì¸ìš©í•´ì£¼ì„¸ìš”.
3. ë²•ë ¹ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì•Œ ìˆ˜ ì—†ë‹¤ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
    model = get_model()
    loop = st.session_state.event_loop
    with ThreadPoolExecutor() as pool:
        res = await loop.run_in_executor(pool, lambda: model.generate_content(prompt))
    return law_name, res.text

# ëª¨ë“  ì—ì´ì „íŠ¸ ë³‘ë ¬ ì‹¤í–‰
async def gather_agent_responses(question, history):
    tasks = [get_law_agent_response_async(name, question, history)
             for name in st.session_state.law_data]
    return await asyncio.gather(*tasks)

# í—¤ë“œ ì—ì´ì „íŠ¸ í†µí•© ë‹µë³€
def get_head_agent_response(responses, question, history):
    combined = "\n\n".join([f"=== {n} ì „ë¬¸ê°€ ë‹µë³€ ===\n{r}" for n, r in responses])
    prompt = f"""
ë‹¹ì‹ ì€ ê´€ì„¸, ì™¸êµ­í™˜ê±°ë˜, ëŒ€ì™¸ë¬´ì—­ë²• ë¶„ì•¼ ì „ë¬¸ì„±ì„ ê°–ì¶˜ ë²•í•™ êµìˆ˜ì´ì ì—¬ëŸ¬ ìë£Œë¥¼ í†µí•©í•˜ì—¬ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{combined}

ì´ì „ ëŒ€í™”:
{history}

ì§ˆë¬¸: {question}

# ì‘ë‹µ ì§€ì¹¨
1 ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¡œë¶€í„° ë°›ì€ ë‹µë³€ì„ ë¶„ì„í•˜ê³  í†µí•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
2. ì œê³µëœ ë²•ë ¹ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
3. ë‹µë³€ì— ì‚¬ìš©í•œ ëª¨ë“  ë²•ë ¹ ì¶œì²˜(ë²•ë ¹ëª…, ì¡°í•­)ë¥¼ ëª…í™•íˆ ì¸ìš©í•´ì£¼ì„¸ìš”.
4. ë²•ë ¹ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì•Œ ìˆ˜ ì—†ë‹¤ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
5. ëª¨ë“  ë‹µë³€ì€ ë‘ê´„ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
"""
    return get_model().generate_content(prompt).text

# --- UI: ì¹´í…Œê³ ë¦¬ ì„ íƒ ---
with st.expander("ì¹´í…Œê³ ë¦¬ ì„ íƒ (ì„ íƒì‚¬í•­)", expanded=True):
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        if st.button("ë¤í•‘ë°©ì§€ê´€ì„¸", use_container_width=True):
            st.session_state.selected_category = "ë¤í•‘ë°©ì§€ê´€ì„¸"
            st.session_state.law_data = load_law_data("ë¤í•‘ë°©ì§€ê´€ì„¸")
            st.session_state.last_used_category = "ë¤í•‘ë°©ì§€ê´€ì„¸"
            st.rerun()
    with c2:
        if st.button("ë¤í•‘íŒì •", use_container_width=True):
            st.session_state.selected_category = "ë¤í•‘íŒì •"
            st.session_state.law_data = load_law_data("ë¤í•‘íŒì •")
            st.session_state.last_used_category = "ë¤í•‘íŒì •"
            st.rerun()
    with c3:
        if st.button("ê´€ë ¨ë²•ë ¹", use_container_width=True):
            st.session_state.selected_category = "ê´€ë ¨ë²•ë ¹"
            st.session_state.law_data = load_law_data("ê´€ë ¨ë²•ë ¹")
            st.session_state.last_used_category = "ê´€ë ¨ë²•ë ¹"
            st.rerun()
    with c4:
        if st.button("AI ìë™ ë¶„ë¥˜", use_container_width=True):
            st.session_state.selected_category = "auto_classify"
            st.session_state.last_used_category = "auto_classify"
            st.rerun()

if st.session_state.selected_category:
    if st.session_state.selected_category == "auto_classify":
        st.info("AIê°€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.")
    else:
        st.info(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {st.session_state.selected_category}")
else:
    st.info("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ê±°ë‚˜ AI ìë™ ë¶„ë¥˜ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”.")

# ëŒ€í™” ê¸°ë¡ ë Œë”ë§
for msg in st.session_state.chat_history:
    with st.chat_message(msg['role']):
        st.markdown(msg['content'])

# ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            # ìë™ ë¶„ë¥˜ ëª¨ë“œì¸ ê²½ìš° ë˜ëŠ” ì„ íƒëœ ì¹´í…Œê³ ë¦¬ê°€ ì—†ëŠ” ê²½ìš°
            if st.session_state.selected_category == "auto_classify" or not st.session_state.selected_category:
                # AIë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = classify_question_category(user_input)
                st.session_state.law_data = load_law_data(category)
                st.write(f"ğŸ” AI ë¶„ì„ ê²°ê³¼: '{category}' ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë˜ì–´ í•´ë‹¹ ë²•ë ¹ì„ ì°¸ì¡°í•©ë‹ˆë‹¤.")
            
            history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.chat_history])
            responses = st.session_state.event_loop.run_until_complete(
                gather_agent_responses(user_input, history)
            )
            answer = get_head_agent_response(responses, user_input, history)
            st.markdown(answer)
            st.session_state.chat_history.append({"role": "assistant", "content": answer})

# ì‚¬ì´ë“œë°” ì•ˆë‚´
with st.sidebar:
    st.markdown("""
### â„¹ï¸ ì‚¬ìš© ì•ˆë‚´

ë‹¤ìŒ ë²•ë ¹ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤:

**ë¤í•‘ë°©ì§€ê´€ì„¸ ê´€ë ¨:**
- ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™
- ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™

**ë¤í•‘íŒì • ê´€ë ¨:**
- ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ìµœì¢…íŒì •ì˜ê²°ì„œ
- ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ìµœì¢…íŒì •ì˜ê²°ì„œ

**ê´€ë ¨ë²•ë ¹:**
- ê´€ì„¸ë²•, ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™
- ë¶ˆê³µì •ë¬´ì—­í–‰ìœ„ ì¡°ì‚¬ ë° ì‚°ì—…í”¼í•´êµ¬ì œì— ê´€í•œ ë²•ë¥ 
""")
    if st.button("ìƒˆ ì±„íŒ… ì‹œì‘", type="primary"):
        st.session_state.chat_history = []
        st.rerun()