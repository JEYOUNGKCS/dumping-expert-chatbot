import streamlit as st                     # ì›¹ ì¸í„°í˜ì´ìŠ¤ ì œì‘ì„ ìœ„í•œ Streamlit
import os                                   # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥ ì‚¬ìš©
import google.generativeai as genai        # Google Gemini AI APIë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥
from pdf_utils import extract_text_from_pdf # PDF ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê¸°ëŠ¥
import asyncio                              # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ asyncio ë¼ì´ë¸ŒëŸ¬ë¦¬
from concurrent.futures import ThreadPoolExecutor  # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor
from sklearn.feature_extraction.text import TfidfVectorizer  # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ê¸° ìœ„í•œ TF-IDF ë„êµ¬
from sklearn.metrics.pairwise import cosine_similarity       # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
from datetime import datetime
import requests                            # ì›¹ ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json                                # JSON ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import time                                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì‹œê°„ ì²˜ë¦¬
import re                                  # ì •ê·œ í‘œí˜„ì‹ì„ ìœ„í•œ re ëª¨ë“ˆ
from google.api_core import exceptions as google_exceptions  # Google API ì˜ˆì™¸ ì²˜ë¦¬
import aiohttp                             # ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ìœ„í•œ aiohttp ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- Streamlit í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì‚¬ì§„í”Œë ˆì´íŠ¸ ë¤í•‘ ì „ë¬¸ê°€ ì±—ë´‡",
    page_icon="ğŸ“‘",
    layout="wide"
)

# --- ë‹µë³€ ìƒì„± ì‹œê°„ ì„¤ì • ---
INITIAL_RESPONSE_TIMEOUT = 10  # ì´ˆê¸° ë‹µë³€ ì œí•œ ì‹œê°„ (ì´ˆ)
FOLLOWUP_RESPONSE_TIMEOUT = 60  # í›„ì† ë‹µë³€ ì œí•œ ì‹œê°„ (ì´ˆ)

# --- ìœ ì €ë¡œë¶€í„° API Key ì…ë ¥ ë°›ê¸° ---
if 'gemini_api_key' not in st.session_state:
    st.session_state.gemini_api_key = ""
if 'serper_api_key' not in st.session_state:
    st.session_state.serper_api_key = ""

with st.sidebar:
    # API í‚¤ ì…ë ¥ ë¶€ë¶„ì„ ë¨¼ì € ë°°ì¹˜
    with st.expander("ğŸ”‘ API Key ì„¤ì •", expanded=True):
        key_input = st.text_input(
            label="Google Gemini API Key ì…ë ¥",
            type="password",
            placeholder="ì—¬ê¸°ì— API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            value=st.session_state.gemini_api_key,
        )
        serper_key_input = st.text_input(
            label="Serper API Key ì…ë ¥",
            type="password",
            placeholder="ì—¬ê¸°ì— Serper API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            value=st.session_state.serper_api_key,
        )
        if key_input:
            st.session_state.gemini_api_key = key_input
        if serper_key_input:
            st.session_state.serper_api_key = serper_key_input

    st.divider()  # êµ¬ë¶„ì„  ì¶”ê°€

    # ì‚¬ìš© ì•ˆë‚´ ë¶€ë¶„
    st.title("ğŸ“š ì‚¬ìš© ì•ˆë‚´")
    st.markdown("""
    ### ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì‚¬ì§„í”Œë ˆì´íŠ¸ ì „ë¬¸ê°€ ì±—ë´‡
    
    ì´ ì±—ë´‡ì€ ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ 
    ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ ê·œì¹™ê³¼ ê´€ë ¨ ë²•ë ¹ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤.
    
    ### ì£¼ìš” ë²•ë ¹ ê·¼ê±°
    - ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ ê·œì¹™
    - ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ìµœì¢…íŒì •
    - ê´€ì„¸ë²• ë° ì‹œí–‰ë ¹
    - ë¶ˆê³µì •ë¬´ì—­í–‰ìœ„ ì¡°ì‚¬ ë° ì‚°ì—…í”¼í•´êµ¬ì œì— ê´€í•œ ë²•ë¥ 
    
    ### ë¬¸ì˜ ê°€ëŠ¥í•œ ì£¼ì œ
    - ë¤í•‘ë°©ì§€ê´€ì„¸ìœ¨ í™•ì¸
    - ê³µê¸‰ìë³„ ì„¸ìœ¨ ì •ë³´
    - ë¤í•‘ íŒì • ë‚´ìš©
    - ê´€ë ¨ ë²•ë ¹ í•´ì„
    - íŠ¹ìˆ˜ê´€ê³„ ê³µê¸‰ì í™•ì¸
    """)

if not st.session_state.gemini_api_key:
    st.sidebar.warning("ì±—ë´‡ì„ ì´ìš©í•˜ë ¤ë©´ API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- Gemini API ì„¤ì • ---
genai.configure(api_key=st.session_state.gemini_api_key)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'law_data' not in st.session_state:
    st.session_state.law_data = {}
# ì„ë² ë”© ìºì‹±ìš© ìƒíƒœ
if 'embedding_data' not in st.session_state:
    st.session_state.embedding_data = {}
# ì´ë²¤íŠ¸ ë£¨í”„ ì´ˆê¸°í™”
if 'event_loop' not in st.session_state:
    st.session_state.event_loop = None
# ë‹µë³€ ìƒì„± ì‹œê°„ ì œì–´
if 'is_followup_question' not in st.session_state:
    st.session_state.is_followup_question = False
if 'last_question_time' not in st.session_state:
    st.session_state.last_question_time = None

# ë‹µë³€ ìƒì„± ì‹œê°„ ì„¤ì •
INITIAL_RESPONSE_TIMEOUT = 10  # ì´ˆê¸° ë‹µë³€ ì œí•œ ì‹œê°„ (ì´ˆ)
FOLLOWUP_RESPONSE_TIMEOUT = 60  # í›„ì† ë‹µë³€ ì œí•œ ì‹œê°„ (ì´ˆ)

# --- ì¹´í…Œê³ ë¦¬ ì •ì˜ ---
LAW_CATEGORIES = {
    "ë¤í•‘ë°©ì§€ê´€ì„¸": {
        "ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™": "docs/ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ì— ëŒ€í•œ ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ì— ê´€í•œ ê·œì¹™(ê¸°íšì¬ì •ë¶€ë ¹)(ì œ00940í˜¸)(20221025).pdf",
    },
    "ë¤í•‘íŒì •": {
        "ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ìµœì¢…íŒì •": "docs/ì¤‘êµ­ì‚° ë”ë¸”ë ˆì´ì–´ ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸_ìµœì¢…íŒì •ì˜ê²°ì„œ.pdf",
    },
    "ê´€ë ¨ë²•ë ¹": {
        "ê´€ì„¸ë²•": "docs/ê´€ì„¸ë²•(ë²•ë¥ )(ì œ20608í˜¸)(20250401).pdf",
        "ê´€ì„¸ë²• ì‹œí–‰ë ¹": "docs/ê´€ì„¸ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹)(ì œ35363í˜¸)(20250722).pdf",
        "ê´€ì„¸ë²• ì‹œí–‰ê·œì¹™": "docs/ê´€ì„¸ë²• ì‹œí–‰ê·œì¹™(ê¸°íšì¬ì •ë¶€ë ¹)(ì œ01110í˜¸)(20250321).pdf",
        "ë¶ˆê³µì •ë¬´ì—­í–‰ìœ„ ì¡°ì‚¬ ë° ì‚°ì—…í”¼í•´êµ¬ì œì— ê´€í•œ ë²•ë¥ ": "docs/ë¶ˆê³µì •ë¬´ì—­í–‰ìœ„ ì¡°ì‚¬ ë° ì‚°ì—…í”¼í•´êµ¬ì œì— ê´€í•œ ë²•ë¥ (ë²•ë¥ )(ì œ20693í˜¸)(20250722).pdf",
    }
}

# ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì •ë³´
CATEGORY_KEYWORDS = {
    "ë¤í•‘ë°©ì§€ê´€ì„¸": ["ë”ë¸”ë ˆì´ì–´", "ì¸ì‡„ì œíŒìš©", "í‰ë©´ëª¨ì–‘", "ì‚¬ì§„í”Œë ˆì´íŠ¸", "ë¤í•‘ë°©ì§€ê´€ì„¸", "ë¤í•‘ë§ˆì§„", "ì •ìƒê°€ê²©", "ìˆ˜ì¶œê°€ê²©", "ë¤í•‘ë¥ "],
    "ë¤í•‘íŒì •": ["ë”ë¸”ë ˆì´ì–´", "ìµœì¢…íŒì •", "ì˜ˆë¹„íŒì •", "ì‚°ì—…í”¼í•´", "ì‹¤ì§ˆì  í”¼í•´", "ì¸ê³¼ê´€ê³„", "êµ­ë‚´ì‚°ì—…", "ì¡°ì‚¬ëŒ€ìƒë¬¼í’ˆ", "ë¤í•‘ìˆ˜ì…"],
    "ê´€ë ¨ë²•ë ¹": ["ê´€ì„¸ë²•", "ì‹œí–‰ë ¹", "ì‹œí–‰ê·œì¹™", "ë¶ˆê³µì •ë¬´ì—­", "ì‚°ì—…í”¼í•´êµ¬ì œ", "ë¬´ì—­ìœ„ì›íšŒ", "ì¡°ì‚¬ì ˆì°¨", "ë¤í•‘ê·œì •"]
}

# ì¹´í…Œê³ ë¦¬ë³„ ìš°ì„ ìˆœìœ„ ì„¤ì •
CATEGORY_PRIORITY = {
    "ë¤í•‘ë°©ì§€ê´€ì„¸": 1,  # ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„
    "ë¤í•‘íŒì •": 2,
    "ê´€ë ¨ë²•ë ¹": 3
}

# ê³µê¸‰ìë³„ ë¤í•‘ë°©ì§€ê´€ì„¸ìœ¨ ì •ë³´
SUPPLIERS_INFO = {
    "MAJOR_SUPPLIERS": {
        "ëŸ¬ì°¨ì´": {
            "name_kr": "ëŸ¬ì°¨ì´",
            "name_en": "Jiangsu Lecai Printing Material Co., Ltd.",
            "rate": 4.10,
            "description": "ëŸ¬ì°¨ì´ ë° ê·¸ ê¸°ì—…ì˜ ì œí’ˆì„ ìˆ˜ì¶œí•˜ëŠ” ì"
        },
        "ì½”ë‹¥": {
            "name_kr": "ì½”ë‹¥",
            "name_en": "Kodak (China) Graphic Communications Company Limited",
            "rate": 3.60,
            "description": "ì½”ë‹¥ê³¼ ê·¸ ê´€ê³„ì‚¬",
            "related_companies": [
                "ì½”ë‹¥ ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸[Kodak (China) Investment Co., Ltd.]",
                "ì½”ë‹¥ ì½”ë¦¬ì•„(Kodak Korea Ltd.)",
                "ì´ìŠ¤íŠ¸ë§Œ ì½”ë‹¥(Eastman Kodak Company)",
                "í™”ê´‘(Lucky Huaguang Graphics Co., Ltd.)",
                "í™”ê´‘ ë‚œì–‘(Lucky Huaguang Nanyang Trading Co., Ltd.)",
                "í™”ê´‘ ë°”ì˜¤ë¦¬(Suzhou Huaguang Baoli Printing Plate Material Co., Ltd.)",
                "ì¢…ì¸(Zhongyin Printing Equipment Co., Ltd.)",
                "ì•„ê·¸íŒŒ í™”ê´‘[Agfa Huaguang (Shanghai) Printing Equipment Co., Ltd.]",
                "í™”í‘¸(Henan Huafu Packaging Technology Co., Ltd.)",
                "ì½”ë‹¥ ì¼ë ‰íŠ¸ë¡œë‹‰[Kodak Electronic Products (Shanghai) Company Limited]"
            ]
        },
        "í™”í‘": {
            "name_kr": "í™”í‘",
            "name_en": "Chongqing Huafeng Dijet Printing Material Co., Ltd.",
            "rate": 7.61,
            "description": "í™”í‘ê³¼ ê·¸ ê´€ê³„ì‚¬",
            "related_companies": [
                "í™”í‘PM(Chongqing Huafeng Printing Material Co., Ltd.)"
            ]
        }
    },
    "OTHER_SUPPLIERS_RATE": 4.87,
    "OTHER_SUPPLIERS_DESCRIPTION": "ê·¸ ë°–ì˜ ê³µê¸‰ì"
}

# API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì„¤ì •
MAX_RETRIES = 3
RETRY_DELAY = 20  # seconds

def get_model_with_retry():
    """
    ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ Gemini ëª¨ë¸ ë°˜í™˜ í•¨ìˆ˜
    """
    for attempt in range(MAX_RETRIES):
        try:
            return genai.GenerativeModel('gemini-2.0-flash')
        except google_exceptions.ResourceExhausted:
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
            else:
                raise

def generate_content_with_retry(model, prompt):
    """
    ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ content ìƒì„± í•¨ìˆ˜
    """
    for attempt in range(MAX_RETRIES):
        try:
            return model.generate_content(prompt)
        except google_exceptions.ResourceExhausted:
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
                continue
            else:
                st.error("API í˜¸ì¶œ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                return None
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None

def get_dumping_rate(supplier_name, product_info=None, special_relationship=None, use_web_search=True):
    """
    ê³µê¸‰ìì˜ ë¤í•‘ë°©ì§€ê´€ì„¸ìœ¨ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        supplier_name (str): ê³µê¸‰ì ì´ë¦„
        product_info (dict, optional): ì œí’ˆ ì •ë³´
        special_relationship (str, optional): íŠ¹ìˆ˜ê´€ê³„ê°€ ìˆëŠ” ì£¼ìš” ê³µê¸‰ì ì´ë¦„
        use_web_search (bool): ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        dict: ì„¸ìœ¨ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    result = {
        "search_date": datetime.now().strftime("%Y-%m-%d"),
        "data_sources": ["Local Database"],
        "is_applicable": True,  # ë¤í•‘ë°©ì§€ê´€ì„¸ ì ìš© ì—¬ë¶€
        "reason": ""
    }
    
    # ì œí’ˆ ì •ë³´ ë¶„ì„
    if product_info:
        product_analysis = analyze_product_info(product_info, use_web_search)
        result["product_analysis"] = product_analysis
        
        # ë¤í•‘ë°©ì§€ê´€ì„¸ ëŒ€ìƒì´ ì•„ë‹Œ ê²½ìš°
        if not product_analysis["is_target_product"]:
            result.update({
                "is_applicable": False,
                "rate": 0,
                "reason": "ë¤í•‘ë°©ì§€ê´€ì„¸ ë¶€ê³¼ëŒ€ìƒ ë¬¼í’ˆì´ ì•„ë‹˜",
                "details": product_analysis["reason"]
            })
            return result
    
    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    if use_web_search and st.session_state.serper_api_key:
        web_info = analyze_web_info(supplier_name, "company")
        if web_info["status"] == "success":
            result["web_search_info"] = web_info
            result["data_sources"].append("Web Search")
    
    # ì£¼ìš” ê³µê¸‰ì ê²€ìƒ‰
    for supplier_key, supplier_info in SUPPLIERS_INFO["MAJOR_SUPPLIERS"].items():
        if (supplier_name.lower() in supplier_info["name_kr"].lower() or 
            supplier_name.lower() in supplier_info["name_en"].lower()):
            result.update({
                "rate": supplier_info["rate"],
                "supplier_type": "major",
                "supplier_info": supplier_info,
                "reason": f"ì£¼ìš” ê³µê¸‰ì {supplier_info['name_kr']}ì— í•´ë‹¹"
            })
            return result
    
    # íŠ¹ìˆ˜ê´€ê³„ ê²€ì‚¬
    if special_relationship:
        for supplier_key, supplier_info in SUPPLIERS_INFO["MAJOR_SUPPLIERS"].items():
            if (special_relationship.lower() in supplier_info["name_kr"].lower() or 
                special_relationship.lower() in supplier_info["name_en"].lower()):
                result.update({
                    "rate": supplier_info["rate"],
                    "supplier_type": "special_relationship",
                    "related_supplier": supplier_info,
                    "reason": f"{supplier_info['name_kr']}ì™€(ê³¼)ì˜ íŠ¹ìˆ˜ê´€ê³„ë¡œ ì¸í•´ í•´ë‹¹ ê³µê¸‰ìì˜ ì„¸ìœ¨ ì ìš©"
                })
                return result
    
    # ê·¸ ë°–ì˜ ê³µê¸‰ì
    result.update({
        "rate": SUPPLIERS_INFO["OTHER_SUPPLIERS_RATE"],
        "supplier_type": "other",
        "reason": "ê·¸ ë°–ì˜ ê³µê¸‰ìì— í•´ë‹¹"
    })
    return result

def search_info(query, api_key, search_type="company"):
    """
    Serper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        query (str): ê²€ìƒ‰ì–´
        api_key (str): Serper API í‚¤
        search_type (str): ê²€ìƒ‰ ìœ í˜• ("company" ë˜ëŠ” "product")
    
    Returns:
        dict: ê²€ìƒ‰ ê²°ê³¼ ì •ë³´
    """
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"{query}_{search_type}"
    if 'search_cache' not in st.session_state:
        st.session_state.search_cache = {}
    
    # ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°˜í™˜ (6ì‹œê°„ ìœ íš¨)
    if cache_key in st.session_state.search_cache:
        cached_result = st.session_state.search_cache[cache_key]
        if (datetime.now() - cached_result['timestamp']).total_seconds() < 21600:  # 6ì‹œê°„
            return cached_result['data']
    
    url = "https://google.serper.dev/search"
    results = []
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
    search_queries = []
    if search_type == "company":
        # ì¤‘êµ­ ê¸°ì—… ì •ë³´ ì‚¬ì´íŠ¸ (ìµœìš°ì„ )
        search_queries.extend([
            f"{query} site:qcc.com",
            f"{query} site:tianyancha.com",
            f"{query} site:cninfo.com.cn"
        ])
        
        # í•œêµ­ ê¸°ì—… ì •ë³´ ì‚¬ì´íŠ¸
        search_queries.extend([
            f"{query} site:dart.fss.or.kr",
            f"{query} site:nicebizinfo.com"
        ])
    else:  # product
        # ì œí’ˆ ê²€ìƒ‰ ìµœì í™”
        search_queries.extend([
            f"{query} ì‚¬ì§„í”Œë ˆì´íŠ¸ PS plate ê·œê²©",
            f"{query} printing plate specifications",
            f"{query} å°åˆ·ç‰ˆ è§„æ ¼"
        ])
    
    # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰ì„ ìœ„í•œ í•¨ìˆ˜
    async def search_query(session, query):
        payload = json.dumps({
            "q": query,
            "num": 5,  # ê²°ê³¼ ìˆ˜ ì œí•œ
            "gl": "kr",
            "hl": "ko"
        })
        headers = {
            'X-API-KEY': api_key,
            'Content-Type': 'application/json'
        }
        try:
            # 20ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            timeout = aiohttp.ClientTimeout(total=20)
            async with session.post(url, headers=headers, data=payload, timeout=timeout) as response:
                result = await response.json()
                if "organic" in result:
                    return result["organic"]
        except asyncio.TimeoutError:
            print(f"Timeout for query: {query}")
            return []
        except Exception as e:
            print(f"Error searching with query '{query}': {str(e)}")
            return []
        return []

    # ë¹„ë™ê¸° ê²€ìƒ‰ ì‹¤í–‰
    async def run_searches():
        # 60ì´ˆ ì „ì²´ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        timeout = aiohttp.ClientTimeout(total=60)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            tasks = [search_query(session, q) for q in search_queries]
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                # ì˜ˆì™¸ ì²˜ë¦¬ ë° ìœ íš¨í•œ ê²°ê³¼ë§Œ ë°˜í™˜
                valid_results = []
                for r in results:
                    if isinstance(r, list):
                        valid_results.extend(r)
                return valid_results
            except asyncio.TimeoutError:
                print("Total search timeout")
                return []

    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
    if not st.session_state.event_loop:
        st.session_state.event_loop = asyncio.new_event_loop()
    
    try:
        results = st.session_state.event_loop.run_until_complete(run_searches())
    except Exception as e:
        print(f"Error in search execution: {str(e)}")
        results = []
    
    # ì¤‘ë³µ ê²°ê³¼ ì œê±° ë° ì •ë ¬
    seen_links = set()
    unique_results = []
    for result in results:
        link = result.get("link", "")
        if link not in seen_links:
            seen_links.add(link)
            # ì¤‘êµ­ ê¸°ì—… ì •ë³´ ì‚¬ì´íŠ¸ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
            priority = 1
            if any(site in link for site in ["qcc.com", "tianyancha.com", "cninfo.com.cn"]):
                priority = 0
            result["priority"] = priority
            unique_results.append(result)
    
    # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬
    unique_results.sort(key=lambda x: (x["priority"], -len(x.get("snippet", ""))))
    
    # ìƒìœ„ 10ê°œ ê²°ê³¼ë§Œ ìœ ì§€
    final_results = unique_results[:10]
    
    # ê²°ê³¼ ìºì‹±
    search_result = {
        "organic": final_results,
        "query": query,
        "search_type": search_type
    }
    
    # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
    if len(st.session_state.search_cache) >= 100:
        oldest_key = min(st.session_state.search_cache.keys(), 
                        key=lambda k: st.session_state.search_cache[k]['timestamp'])
        del st.session_state.search_cache[oldest_key]
    
    st.session_state.search_cache[cache_key] = {
        'data': search_result,
        'timestamp': datetime.now()
    }
    
    return search_result

def analyze_web_info(query, search_type="company"):
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        query (str): ê²€ìƒ‰ì–´
        search_type (str): ê²€ìƒ‰ ìœ í˜• ("company" ë˜ëŠ” "product")
    
    Returns:
        dict: ë¶„ì„ëœ ì •ë³´
    """
    if not st.session_state.serper_api_key:
        return {
            "status": "error",
            "message": "Serper API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "search_date": datetime.now().strftime("%Y-%m-%d")
        }
    
    search_results = search_info(query, st.session_state.serper_api_key, search_type)
    
    if "error" in search_results:
        return {
            "status": "error",
            "message": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {search_results['error']}",
            "search_date": datetime.now().strftime("%Y-%m-%d")
        }
    
    result_info = {
        "query": query,
        "search_date": datetime.now().strftime("%Y-%m-%d"),
        "status": "success",
        "company_details": {
            "basic_info": {
                "company_name": "",
                "company_name_en": "",
                "establishment_date": "",
                "business_number": "",
                "representative": "",
                "website": "",
                "contact": "",
                "main_business": []
            },
            "addresses": [],            # ì£¼ì†Œ ì •ë³´
            "registration": [],         # ì‚¬ì—…ì/ë²•ì¸ ë“±ë¡ ì •ë³´
            "shareholders": [],         # ì£¼ì£¼ ì •ë³´
            "subsidiaries": [],         # ìíšŒì‚¬ ì •ë³´
            "parent_companies": [],     # ëª¨íšŒì‚¬ ì •ë³´
            "business_scope": [],       # ì‚¬ì—… ë²”ìœ„
            "trade_info": [],          # ë¬´ì—­ ì •ë³´
            "financial_info": [],      # ì¬ë¬´ ì •ë³´
            "certifications": []       # ì¸ì¦ ì •ë³´
        },
        "special_relationships": [],    # íŠ¹ìˆ˜ê´€ê³„ ì •ë³´
        "news_and_updates": [],        # ìµœì‹  ë‰´ìŠ¤ ë° ì—…ë°ì´íŠ¸
        "raw_search_results": search_results
    }
    
    try:
        for result in search_results.get("organic", []):
            title = result.get("title", "")
            snippet = result.get("snippet", "")
            link = result.get("link", "")
            combined_text = (title + " " + snippet).lower()
            
            if search_type == "company":
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                if any(keyword in combined_text 
                      for keyword in ["íšŒì‚¬ì†Œê°œ", "ê¸°ì—…ê°œìš”", "company profile", "about us", 
                                    "ä¼ä¸šç®€ä»‹", "å…¬å¸ç®€ä»‹"]):
                    # ëŒ€í‘œìëª… ì¶”ì¶œ
                    rep_match = re.search(r"ëŒ€í‘œ[ìì´ì‚¬]*[:\s]*([\w\s]+)", snippet)
                    if rep_match:
                        result_info["company_details"]["basic_info"]["representative"] = rep_match.group(1).strip()
                    
                    # ì„¤ë¦½ì¼ ì¶”ì¶œ
                    date_match = re.search(r"ì„¤ë¦½[ì¼ì]*[:\s]*(\d{4}[ë…„\s]*\d{1,2}[ì›”\s]*\d{1,2}[ì¼]*)", snippet)
                    if date_match:
                        result_info["company_details"]["basic_info"]["establishment_date"] = date_match.group(1)
                    
                    # ì‚¬ì—…ìë²ˆí˜¸ ì¶”ì¶œ
                    biz_match = re.search(r"ì‚¬ì—…ì[ë“±ë¡]*ë²ˆí˜¸[:\s]*(\d{3}-\d{2}-\d{5})", snippet)
                    if biz_match:
                        result_info["company_details"]["basic_info"]["business_number"] = biz_match.group(1)
                
                # ì£¼ì†Œ ì •ë³´
                if any(keyword in combined_text 
                      for keyword in ["ì£¼ì†Œ", "ì†Œì¬ì§€", "address", "location", "æ‰€åœ¨åœ°", "åœ°å€"]):
                    result_info["company_details"]["addresses"].append({
                        "address": snippet,
                        "source": link,
                        "type": "ë³¸ì‚¬" if "ë³¸ì‚¬" in combined_text else "ì‚¬ì—…ì¥"
                    })
                
                # ì£¼ì£¼ ì •ë³´
                if any(keyword in combined_text 
                      for keyword in ["ì£¼ì£¼", "ì§€ë¶„", "ì¶œì", "shareholder", "è‚¡ä¸œ", "æŒè‚¡"]):
                    result_info["company_details"]["shareholders"].append({
                        "info": snippet,
                        "source": link,
                        "date_found": datetime.now().strftime("%Y-%m-%d")
                    })
                
                # ìíšŒì‚¬/ê³„ì—´ì‚¬ ì •ë³´
                if any(keyword in combined_text 
                      for keyword in ["ìíšŒì‚¬", "ê³„ì—´ì‚¬", "subsidiary", "affiliate", "å­å…¬å¸", "å…³è”å…¬å¸"]):
                    result_info["company_details"]["subsidiaries"].append({
                        "info": snippet,
                        "source": link,
                        "date_found": datetime.now().strftime("%Y-%m-%d")
                    })
                
                # ëª¨íšŒì‚¬ ì •ë³´
                if any(keyword in combined_text 
                      for keyword in ["ëª¨íšŒì‚¬", "ì§€ì£¼íšŒì‚¬", "parent company", "holding company", 
                                    "æ¯å…¬å¸", "æ§è‚¡å…¬å¸"]):
                    result_info["company_details"]["parent_companies"].append({
                        "info": snippet,
                        "source": link,
                        "date_found": datetime.now().strftime("%Y-%m-%d")
                    })
                
                # ì‚¬ì—… ë²”ìœ„
                if any(keyword in combined_text 
                      for keyword in ["ì‚¬ì—…ì˜ì—­", "ì£¼ìš”ì œí’ˆ", "business scope", "products", 
                                    "ç»è¥èŒƒå›´", "ä¸»è¥äº§å“"]):
                    result_info["company_details"]["business_scope"].append({
                        "info": snippet,
                        "source": link,
                        "date_found": datetime.now().strftime("%Y-%m-%d")
                    })
                
                # ë¬´ì—­ ì •ë³´
                if any(keyword in combined_text 
                      for keyword in ["ìˆ˜ì¶œ", "ìˆ˜ì…", "ë¬´ì—­", "export", "import", "trade", 
                                    "è¿›å‡ºå£", "è´¸æ˜“"]):
                    result_info["company_details"]["trade_info"].append({
                        "info": snippet,
                        "source": link,
                        "date_found": datetime.now().strftime("%Y-%m-%d")
                    })
                
                # ì¬ë¬´ ì •ë³´
                if any(keyword in combined_text 
                      for keyword in ["ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ë‹¹ê¸°ìˆœì´ìµ", "revenue", "profit", 
                                    "è¥æ”¶", "åˆ©æ¶¦"]):
                    result_info["company_details"]["financial_info"].append({
                        "info": snippet,
                        "source": link,
                        "date_found": datetime.now().strftime("%Y-%m-%d")
                    })
                
                # ì¸ì¦ ì •ë³´
                if any(keyword in combined_text 
                      for keyword in ["ì¸ì¦", "íŠ¹í—ˆ", "certification", "patent", 
                                    "è®¤è¯", "ä¸“åˆ©"]):
                    result_info["company_details"]["certifications"].append({
                        "info": snippet,
                        "source": link,
                        "date_found": datetime.now().strftime("%Y-%m-%d")
                    })
            
            # ìµœì‹  ë‰´ìŠ¤ ë° ì—…ë°ì´íŠ¸
            if any(keyword in combined_text
                  for keyword in ["ë‰´ìŠ¤", "ê³µì‹œ", "ë³´ë„", "news", "update", "announcement", 
                                "æ–°é—»", "å…¬å‘Š"]):
                result_info["news_and_updates"].append({
                    "title": title,
                    "content": snippet,
                    "source": link,
                    "date_found": datetime.now().strftime("%Y-%m-%d")
                })
    
    except Exception as e:
        result_info["analysis_error"] = str(e)
    
    # íŠ¹ìˆ˜ê´€ê³„ ë¶„ì„
    if search_type == "company":
        special_relationship = check_special_relationship({"name": query})
        if special_relationship["has_special_relationship"]:
            result_info["special_relationships"] = special_relationship["relationships"]
    
    return result_info

def format_company_info(company_info):
    """
    ê¸°ì—… ì •ë³´ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        company_info (dict): analyze_web_info í•¨ìˆ˜ì˜ ê²°ê³¼
    
    Returns:
        str: í¬ë§·íŒ…ëœ ê¸°ì—… ì •ë³´
    """
    if company_info["status"] != "success":
        return f"ê¸°ì—… ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {company_info.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
    
    formatted_info = []
    formatted_info.append(f"## ğŸ¢ {company_info['query']} ê¸°ì—… ì •ë³´")
    formatted_info.append(f"*ê²€ìƒ‰ ì¼ì: {company_info['search_date']}*")
    formatted_info.append("---")
    
    # ê¸°ë³¸ ì •ë³´
    basic_info = company_info["company_details"]["basic_info"]
    if any(basic_info.values()):
        formatted_info.append("### ğŸ“‹ ê¸°ë³¸ ì •ë³´")
        if basic_info["company_name"]: 
            formatted_info.append(f"- ê¸°ì—…ëª…: {basic_info['company_name']}")
        if basic_info["company_name_en"]: 
            formatted_info.append(f"- ì˜ë¬¸ëª…: {basic_info['company_name_en']}")
        if basic_info["establishment_date"]: 
            formatted_info.append(f"- ì„¤ë¦½ì¼: {basic_info['establishment_date']}")
        if basic_info["business_number"]: 
            formatted_info.append(f"- ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸: {basic_info['business_number']}")
        if basic_info["representative"]: 
            formatted_info.append(f"- ëŒ€í‘œì: {basic_info['representative']}")
        formatted_info.append("")
    
    # ì£¼ì†Œ ì •ë³´
    if company_info["company_details"]["addresses"]:
        formatted_info.append("### ğŸ“ ì‚¬ì—…ì¥ ì •ë³´")
        for addr in company_info["company_details"]["addresses"]:
            formatted_info.append(f"- {addr['type']}: {addr['address']}")
        formatted_info.append("")
    
    # ì£¼ì£¼ ë° ì§€ë¶„ ì •ë³´
    if company_info["company_details"]["shareholders"]:
        formatted_info.append("### ğŸ‘¥ ì£¼ì£¼ ë° ì§€ë¶„ ì •ë³´")
        for shareholder in company_info["company_details"]["shareholders"]:
            formatted_info.append(f"- {shareholder['info']}")
        formatted_info.append("")
    
    # ìíšŒì‚¬/ê³„ì—´ì‚¬ ì •ë³´
    if company_info["company_details"]["subsidiaries"]:
        formatted_info.append("### ğŸ”„ ìíšŒì‚¬/ê³„ì—´ì‚¬ ì •ë³´")
        for subsidiary in company_info["company_details"]["subsidiaries"]:
            formatted_info.append(f"- {subsidiary['info']}")
        formatted_info.append("")
    
    # ëª¨íšŒì‚¬ ì •ë³´
    if company_info["company_details"]["parent_companies"]:
        formatted_info.append("### â¬†ï¸ ëª¨íšŒì‚¬ ì •ë³´")
        for parent in company_info["company_details"]["parent_companies"]:
            formatted_info.append(f"- {parent['info']}")
        formatted_info.append("")
    
    # ì‚¬ì—… ë²”ìœ„
    if company_info["company_details"]["business_scope"]:
        formatted_info.append("### ğŸ¯ ì‚¬ì—… ë²”ìœ„")
        for scope in company_info["company_details"]["business_scope"]:
            formatted_info.append(f"- {scope['info']}")
        formatted_info.append("")
    
    # ë¬´ì—­ ì •ë³´
    if company_info["company_details"]["trade_info"]:
        formatted_info.append("### ğŸŒ ë¬´ì—­ í™œë™")
        for trade in company_info["company_details"]["trade_info"]:
            formatted_info.append(f"- {trade['info']}")
        formatted_info.append("")
    
    # ì¬ë¬´ ì •ë³´
    if company_info["company_details"]["financial_info"]:
        formatted_info.append("### ğŸ’° ì¬ë¬´ ì •ë³´")
        for finance in company_info["company_details"]["financial_info"]:
            formatted_info.append(f"- {finance['info']}")
        formatted_info.append("")
    
    # ì¸ì¦ ì •ë³´
    if company_info["company_details"]["certifications"]:
        formatted_info.append("### ğŸ“œ ì¸ì¦ ë° íŠ¹í—ˆ")
        for cert in company_info["company_details"]["certifications"]:
            formatted_info.append(f"- {cert['info']}")
        formatted_info.append("")
    
    # íŠ¹ìˆ˜ê´€ê³„ ì •ë³´
    if company_info.get("special_relationships"):
        formatted_info.append("### âš ï¸ ë¤í•‘ë°©ì§€ê´€ì„¸ ëŒ€ìƒ ê¸°ì—…ê³¼ì˜ íŠ¹ìˆ˜ê´€ê³„")
        for relationship in company_info["special_relationships"]:
            formatted_info.append(f"- ê´€ë ¨ ê¸°ì—…: **{relationship['major_supplier']}**")
            formatted_info.append(f"  - ì‹ ë¢°ë„ ì ìˆ˜: {relationship['confidence_score']:.2f}")
            for rel in relationship["relationships_found"]:
                formatted_info.append(f"  - {rel['description']}: {rel['detail']}")
        formatted_info.append("")
    
    # ìµœì‹  ë‰´ìŠ¤
    if company_info["news_and_updates"]:
        formatted_info.append("### ğŸ“° ìµœì‹  ì†Œì‹")
        for news in company_info["news_and_updates"][:5]:  # ìµœê·¼ 5ê°œë§Œ í‘œì‹œ
            formatted_info.append(f"- {news['title']}")
            formatted_info.append(f"  {news['content']}")
        formatted_info.append("")
    
    return "\n".join(formatted_info)

def check_special_relationship(company_info, use_web_search=True):
    """
    íŠ¹ìˆ˜ê´€ê³„ ì—¬ë¶€ë¥¼ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        company_info (dict): íšŒì‚¬ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        use_web_search (bool): ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        dict: íŠ¹ìˆ˜ê´€ê³„ ë¶„ì„ ê²°ê³¼
    """
    relationships = []
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"special_relationship_{company_info['name']}"
    if 'relationship_cache' not in st.session_state:
        st.session_state.relationship_cache = {}
    
    # ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°˜í™˜
    if cache_key in st.session_state.relationship_cache:
        cached_result = st.session_state.relationship_cache[cache_key]
        # ìºì‹œ ìœ íš¨ê¸°ê°„ í™•ì¸ (24ì‹œê°„)
        if (datetime.now() - cached_result['timestamp']).total_seconds() < 86400:
            return cached_result['data']
    
    # ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
    web_info = None
    if use_web_search and st.session_state.serper_api_key:
        web_info = analyze_web_info(company_info["name"], "company")
    
    # ì£¼ìš” ê³µê¸‰ìë“¤ê³¼ì˜ ê´€ê³„ ê²€ì‚¬
    for supplier_key, supplier_info in SUPPLIERS_INFO["MAJOR_SUPPLIERS"].items():
        relationship = {
            "major_supplier": supplier_info["name_kr"],
            "major_supplier_en": supplier_info["name_en"],
            "relationships_found": [],
            "confidence_score": 0.0,  # ê´€ê³„ ì‹ ë¢°ë„ ì ìˆ˜
            "evidence": []  # ì¦ê±° ìë£Œ ì €ì¥
        }
        
        # 1. ê¸°ë³¸ ê²€ì‚¬ (íšŒì‚¬ëª… ìœ ì‚¬ì„±)
        name_similarity = calculate_name_similarity(company_info["name"], supplier_info)
        if name_similarity > 0.7:
            relationship["relationships_found"].append({
                "type": "name_similarity",
                "description": "íšŒì‚¬ëª… ìœ ì‚¬ì„± ë°œê²¬",
                "confidence": name_similarity,
                "details": f"ìœ ì‚¬ë„ ì ìˆ˜: {name_similarity:.2f}"
            })
            relationship["confidence_score"] += name_similarity
            relationship["evidence"].append({
                "type": "name_match",
                "source": "ê¸°ì—…ëª… ë¶„ì„",
                "details": f"ê²€ì‚¬ ëŒ€ìƒ: {company_info['name']}, ì£¼ìš” ê³µê¸‰ì: {supplier_info['name_kr']}/{supplier_info['name_en']}"
            })
        
        # 2. ì›¹ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
        if web_info and web_info["status"] == "success":
            # 2.1 ì£¼ì†Œ ì •ë³´ ë¶„ì„
            for address in web_info["company_details"]["addresses"]:
                address_similarity = calculate_address_similarity(address["address"], supplier_info.get("address", ""))
                if address_similarity > 0.8:
                    relationship["relationships_found"].append({
                        "type": "address_match",
                        "description": "ì£¼ì†Œ ì¼ì¹˜",
                        "confidence": address_similarity,
                        "source": address.get("source", ""),
                        "detail": address["address"]
                    })
                    relationship["confidence_score"] += address_similarity
                    relationship["evidence"].append({
                        "type": "address_match",
                        "source": address.get("source", "ì£¼ì†Œ ì •ë³´ ë¶„ì„"),
                        "details": f"ì¼ì¹˜ ì ìˆ˜: {address_similarity:.2f}"
                    })
            
            # 2.2 ì£¼ì£¼/ì§€ë¶„ ê´€ê³„ ë¶„ì„
            for shareholder in web_info["company_details"]["shareholders"]:
                if analyze_shareholder_relationship(shareholder, supplier_info):
                    relationship["relationships_found"].append({
                        "type": "shareholding",
                        "description": "ì£¼ì£¼/ì§€ë¶„ ê´€ê³„ ë°œê²¬",
                        "confidence": 0.9,
                        "source": shareholder.get("source", ""),
                        "detail": shareholder.get("description", "")
                    })
                    relationship["confidence_score"] += 0.9
                    relationship["evidence"].append({
                        "type": "shareholding",
                        "source": shareholder.get("source", "ì£¼ì£¼ ì •ë³´ ë¶„ì„"),
                        "details": shareholder.get("description", "")
                    })
            
            # 2.3 ìíšŒì‚¬/ëª¨íšŒì‚¬ ê´€ê³„ ë¶„ì„
            for subsidiary in web_info["company_details"]["subsidiaries"]:
                if analyze_company_relationship(subsidiary, supplier_info):
                    relationship["relationships_found"].append({
                        "type": "subsidiary",
                        "description": "ìíšŒì‚¬ ê´€ê³„ ë°œê²¬",
                        "confidence": 0.9,
                        "source": subsidiary.get("source", ""),
                        "detail": subsidiary.get("description", "")
                    })
                    relationship["confidence_score"] += 0.9
                    relationship["evidence"].append({
                        "type": "subsidiary",
                        "source": subsidiary.get("source", "ìíšŒì‚¬ ì •ë³´ ë¶„ì„"),
                        "details": subsidiary.get("description", "")
                    })
            
            for parent in web_info["company_details"]["parent_companies"]:
                if analyze_company_relationship(parent, supplier_info):
                    relationship["relationships_found"].append({
                        "type": "parent_company",
                        "description": "ëª¨íšŒì‚¬ ê´€ê³„ ë°œê²¬",
                        "confidence": 0.9,
                        "source": parent.get("source", ""),
                        "detail": parent.get("description", "")
                    })
                    relationship["confidence_score"] += 0.9
                    relationship["evidence"].append({
                        "type": "parent_company",
                        "source": parent.get("source", "ëª¨íšŒì‚¬ ì •ë³´ ë¶„ì„"),
                        "details": parent.get("description", "")
                    })
        
        # ê´€ê³„ê°€ ë°œê²¬ë˜ê³  ì‹ ë¢°ë„ê°€ ì¶©ë¶„í•œ ê²½ìš°ì—ë§Œ ê²°ê³¼ì— ì¶”ê°€
        if relationship["relationships_found"]:
            # ì‹ ë¢°ë„ ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„)
            relationship["confidence_score"] = min(1.0, relationship["confidence_score"] / len(relationship["relationships_found"]))
            # ë†’ì€ ì‹ ë¢°ë„(0.7 ì´ìƒ) ê´€ê³„ë§Œ í¬í•¨
            if relationship["confidence_score"] >= 0.7:
                # ë‚®ì€ ì‹ ë¢°ë„ ê´€ê³„ëŠ” ì œì™¸
                relationship["relationships_found"] = [
                    r for r in relationship["relationships_found"]
                    if r["confidence"] >= 0.7
                ]
                if relationship["relationships_found"]:  # ë†’ì€ ì‹ ë¢°ë„ ê´€ê³„ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    relationships.append(relationship)
    
    result = {
        "has_special_relationship": len(relationships) > 0,
        "relationships": relationships,
        "analysis_date": datetime.now().strftime("%Y-%m-%d"),
        "data_sources": ["Local Database"]
    }
    
    if web_info:
        result["data_sources"].append("Web Search")
    
    # íŠ¹ìˆ˜ê´€ê³„ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìƒì„¸ ì •ë³´ í¬í•¨
    if result["has_special_relationship"]:
        high_confidence_relationships = [r for r in relationships if r["confidence_score"] >= 0.8]
        result.update({
            "high_confidence_relationship": len(high_confidence_relationships) > 0,
            "relationship_summary": {
                "total_relationships": len(relationships),
                "high_confidence_relationships": len(high_confidence_relationships),
                "highest_confidence_score": max([r["confidence_score"] for r in relationships])
            }
        })
        # ì›¹ ê²€ìƒ‰ ì •ë³´ëŠ” ìœ ì˜ë¯¸í•œ íŠ¹ìˆ˜ê´€ê³„ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í¬í•¨
        if web_info:
            result["web_search_info"] = web_info
    else:
        # íŠ¹ìˆ˜ê´€ê³„ê°€ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ê²°ê³¼ë§Œ ë°˜í™˜
        result = {
            "has_special_relationship": False,
            "analysis_date": datetime.now().strftime("%Y-%m-%d"),
            "message": "ìœ ì˜ë¯¸í•œ íŠ¹ìˆ˜ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }
    
    # ê²°ê³¼ ìºì‹±
    st.session_state.relationship_cache[cache_key] = {
        'data': result,
        'timestamp': datetime.now()
    }
    
    return result

def calculate_name_similarity(name1, supplier_info):
    """
    íšŒì‚¬ëª… ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    name1 = name1.lower()
    name_kr = supplier_info["name_kr"].lower()
    name_en = supplier_info["name_en"].lower()
    
    # ì •í™•í•œ ì¼ì¹˜ ê²€ì‚¬
    if name1 in name_kr or name_kr in name1:
        return 1.0
    if name1 in name_en or name_en in name1:
        return 1.0
    
    # ë¶€ë¶„ ì¼ì¹˜ ê²€ì‚¬
    name1_words = set(re.findall(r'\w+', name1))
    name_kr_words = set(re.findall(r'\w+', name_kr))
    name_en_words = set(re.findall(r'\w+', name_en))
    
    # í•œê¸€ ì´ë¦„ ìœ ì‚¬ë„
    kr_similarity = len(name1_words & name_kr_words) / max(len(name1_words), len(name_kr_words))
    # ì˜ë¬¸ ì´ë¦„ ìœ ì‚¬ë„
    en_similarity = len(name1_words & name_en_words) / max(len(name1_words), len(name_en_words))
    
    return max(kr_similarity, en_similarity)

def calculate_address_similarity(addr1, addr2):
    """
    ì£¼ì†Œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    if not addr1 or not addr2:
        return 0.0
    
    addr1 = addr1.lower()
    addr2 = addr2.lower()
    
    # ì •í™•í•œ ì¼ì¹˜ ê²€ì‚¬
    if addr1 == addr2:
        return 1.0
    
    # ë¶€ë¶„ ì¼ì¹˜ ê²€ì‚¬
    addr1_words = set(re.findall(r'\w+', addr1))
    addr2_words = set(re.findall(r'\w+', addr2))
    
    # ì£¼ì†Œ ìœ ì‚¬ë„
    similarity = len(addr1_words & addr2_words) / max(len(addr1_words), len(addr2_words))
    
    return similarity

def analyze_shareholder_relationship(shareholder, supplier_info):
    """
    ì£¼ì£¼ ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    description = shareholder.get("description", "").lower()
    name_kr = supplier_info["name_kr"].lower()
    name_en = supplier_info["name_en"].lower()
    
    # ì£¼ìš” í‚¤ì›Œë“œ
    keywords = ["ì£¼ì£¼", "ì§€ë¶„", "ì¶œì", "shareholder", "stake", "ownership", "è‚¡ä¸œ", "æŒè‚¡"]
    
    if any(keyword in description for keyword in keywords):
        if name_kr in description or name_en in description:
            return True
    
    return False

def analyze_company_relationship(company, supplier_info):
    """
    íšŒì‚¬ ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    description = company.get("description", "").lower()
    name_kr = supplier_info["name_kr"].lower()
    name_en = supplier_info["name_en"].lower()
    
    # ì£¼ìš” í‚¤ì›Œë“œ
    keywords = ["ìíšŒì‚¬", "ê³„ì—´ì‚¬", "ëª¨íšŒì‚¬", "ì§€ì£¼íšŒì‚¬", "subsidiary", "affiliate", "parent", 
               "å­å…¬å¸", "å…³è”å…¬å¸", "æ¯å…¬å¸", "æ§è‚¡å…¬å¸"]
    
    if any(keyword in description for keyword in keywords):
        if name_kr in description or name_en in description:
            return True
    
    return False

def analyze_product_info(product_info, use_web_search=True):
    """
    ì œí’ˆ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ë¤í•‘ë°©ì§€ê´€ì„¸ ëŒ€ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        product_info (dict): ì œí’ˆ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        use_web_search (bool): ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        dict: ì œí’ˆ ë¶„ì„ ê²°ê³¼
    """
    result = {
        "analysis_date": datetime.now().strftime("%Y-%m-%d"),
        "data_sources": ["Local Database"],
        "product_name": product_info.get("name", ""),
        "model": product_info.get("model", ""),
        "specifications": product_info.get("specifications", {}),
        "is_target_product": False,
        "reason": "",
        "dumping_duty_info": None
    }
    
    # ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
    if use_web_search and st.session_state.serper_api_key:
        search_query = f"{result['product_name']} {result.get('model', '')}".strip()
        web_info = analyze_web_info(search_query, "product")
        
        if web_info and web_info["status"] == "success":
            result["data_sources"].append("Web Search")
            result["web_search_info"] = web_info
            
            # ê¸°ìˆ  ì‚¬ì–‘ ì •ë³´ ì¶”ê°€
            if web_info.get("technical_specs"):
                result["specifications"].update({
                    "web_found_specs": web_info["technical_specs"]
                })
            
            # ë¤í•‘ë°©ì§€ê´€ì„¸ ê´€ë ¨ ì •ë³´ ì¶”ê°€
            if web_info.get("dumping_duty_info"):
                result["dumping_duty_info"] = web_info["dumping_duty_info"]
    
    # ì œí’ˆì´ ë¤í•‘ë°©ì§€ê´€ì„¸ ëŒ€ìƒì¸ì§€ íŒë‹¨
    # 1. ì œí’ˆëª… ê¸°ë°˜ ê²€ì‚¬
    product_keywords = ["ì¸ì‡„ì œíŒìš©", "í‰ë©´ëª¨ì–‘", "ì‚¬ì§„í”Œë ˆì´íŠ¸", "printing plate", "PS plate"]
    if any(keyword in str(result["product_name"]).lower() for keyword in product_keywords):
        result["is_target_product"] = True
        result["reason"] = "ì œí’ˆëª…ì—ì„œ ëŒ€ìƒ í’ˆëª© í‚¤ì›Œë“œ ë°œê²¬"
    
    # 2. ì‚¬ì–‘ ê¸°ë°˜ ê²€ì‚¬
    if result["specifications"]:
        spec_keywords = ["PS", "presensitized", "ê°ê´‘", "ì•Œë£¨ë¯¸ëŠ„", "aluminum", "offset"]
        spec_text = str(result["specifications"]).lower()
        if any(keyword in spec_text for keyword in spec_keywords):
            result["is_target_product"] = True
            result["reason"] = "ì œí’ˆ ì‚¬ì–‘ì—ì„œ ëŒ€ìƒ í’ˆëª© íŠ¹ì„± ë°œê²¬"
    
    # 3. ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ê²€ì‚¬
    if result.get("web_search_info"):
        if result["web_search_info"].get("dumping_duty_info"):
            result["is_target_product"] = True
            result["reason"] = "ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¤í•‘ë°©ì§€ê´€ì„¸ ëŒ€ìƒ í™•ì¸"
    
    return result

# PDF ë¡œë“œ ë° ì„ë² ë”© ìƒì„± í•¨ìˆ˜
@st.cache_data
def load_law_data(category=None):
    law_data = {}
    missing_files = []
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ íŒŒì¼ì„ í•œë²ˆì— ë¡œë“œ
    pdf_files = {}
    for cat_files in LAW_CATEGORIES.values():
        pdf_files.update(cat_files)
    
    for law_name, pdf_path in pdf_files.items():
        if os.path.exists(pdf_path):
            text = extract_text_from_pdf(pdf_path)
            law_data[law_name] = text
            # ì„ë² ë”© ìƒì„± ë° ìºì‹±
            vec, mat, chunks = create_embeddings_for_text(text)
            st.session_state.embedding_data[law_name] = (vec, mat, chunks)
        else:
            missing_files.append(pdf_path)
    if missing_files:
        st.warning(f"ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
    return law_data

# ì„ë² ë”© ë° ì²­í¬ ìƒì„±
@st.cache_data
def create_embeddings_for_text(text, chunk_size=1000):
    chunks = []
    step = chunk_size // 2
    for i in range(0, len(text), step):
        segment = text[i:i+chunk_size]
        if len(segment) > 100:
            chunks.append(segment)
    vectorizer = TfidfVectorizer()
    matrix = vectorizer.fit_transform(chunks)
    return vectorizer, matrix, chunks

# ì¿¼ë¦¬ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
def search_relevant_chunks(query, vectorizer, tfidf_matrix, text_chunks, top_k=3, threshold=0.005):
    q_vec = vectorizer.transform([query])
    sims = cosine_similarity(q_vec, tfidf_matrix).flatten()
    indices = sims.argsort()[-top_k:][::-1]
    sel = [text_chunks[i] for i in indices if sims[i] > threshold]
    if not sel:
        sel = [text_chunks[i] for i in indices]
    return "\n\n".join(sel)

# Gemini ëª¨ë¸ ë°˜í™˜ í•¨ìˆ˜ ìˆ˜ì •
def get_model():
    return get_model_with_retry()

def summarize_pdf_content(pdf_path, chunk_size=3000):
    """
    PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ
        chunk_size (int): ê° ì²­í¬ì˜ í¬ê¸°
    
    Returns:
        str: ìš”ì•½ëœ ë‚´ìš©
    """
    try:
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        full_text = extract_text_from_pdf(pdf_path)
        if not full_text:
            return "PDF ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = []
        for i in range(0, len(full_text), chunk_size):
            chunk = full_text[i:i + chunk_size]
            if len(chunk.strip()) > 100:  # ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ” ì²­í¬ë§Œ í¬í•¨
                chunks.append(chunk)

        # ê° ì²­í¬ ìš”ì•½
        model = get_model()
        summaries = []
        
        for chunk in chunks:
            prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•˜ë˜, ì¤‘ìš”í•œ ìˆ˜ì¹˜ë‚˜ ê²°ì •ì‚¬í•­ì€ ë°˜ë“œì‹œ í¬í•¨í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{chunk}

ìš”ì•½ í˜•ì‹:
- bullet point í˜•ì‹ìœ¼ë¡œ ì‘ì„±
- ê° ìš”ì ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ì œí•œ
- ì¤‘ìš” ìˆ˜ì¹˜ì™€ ê²°ì •ì‚¬í•­ ê°•ì¡°
"""
            result = generate_content_with_retry(model, prompt)
            if result:
                summaries.append(result.text)
            time.sleep(1)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€

        # ì „ì²´ ìš”ì•½ ìƒì„±
        if not summaries:
            return "ë¬¸ì„œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        final_summary_prompt = f"""
ë‹¤ìŒì€ ë¬¸ì„œì˜ ê° ë¶€ë¶„ ìš”ì•½ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ê° ë¶€ë¶„ ìš”ì•½:
{' '.join(summaries)}

ìš”ì•½ í˜•ì‹:
1. ë¬¸ì„œ ê°œìš” (1-2ë¬¸ì¥)
2. ì£¼ìš” ê²°ì •ì‚¬í•­ (bullet points)
3. ì¤‘ìš” ìˆ˜ì¹˜ ë° ë°ì´í„° (bullet points)
4. ê²°ë¡  (1-2ë¬¸ì¥)
"""
        final_result = generate_content_with_retry(model, final_summary_prompt)
        return final_result.text if final_result else "ìµœì¢… ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ë¹ ë¥¸ ìš”ì•½ ìƒì„± í•¨ìˆ˜
def generate_quick_summary(responses, question):
    """
    ìˆ˜ì§‘ëœ ì‘ë‹µë“¤ì„ ë¹ ë¥´ê²Œ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    """
    if not responses:
        return get_quick_response(question)
    
    # ì‘ë‹µì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    scored_responses = []
    for law_name, response in responses:
        # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ ê³„ì‚°
        question_keywords = set(re.findall(r'\w+', question.lower()))
        response_text = response.lower()
        matched_keywords = sum(1 for keyword in question_keywords 
                             if len(keyword) > 1 and keyword in response_text)
        relevance_score = matched_keywords / len(question_keywords) if question_keywords else 0
        
        # ì‘ë‹µ ê¸¸ì´ë„ ì ìˆ˜ì— ë°˜ì˜ (ë„ˆë¬´ ì§§ì€ ì‘ë‹µì€ ì œì™¸)
        length_score = min(len(response) / 1000, 1.0)  # 1000ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
        
        total_score = (relevance_score * 0.7) + (length_score * 0.3)  # ê´€ë ¨ì„± 70%, ê¸¸ì´ 30% ë°˜ì˜
        scored_responses.append((law_name, response, total_score))
    
    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    scored_responses.sort(key=lambda x: x[2], reverse=True)
    
    # ìƒìœ„ 2ê°œ ì‘ë‹µë§Œ ì„ íƒ
    top_responses = scored_responses[:2]
    
    # ìš”ì•½ ìƒì„±
    summary_parts = []
    for law_name, response, score in top_responses:
        # ì‘ë‹µì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì²˜ìŒ 2-3ë¬¸ì¥)
        sentences = re.split(r'[.!?]\s+', response)
        key_sentences = sentences[:3]
        summary = '. '.join(key_sentences) + '.'
        
        summary_parts.append(f"[{law_name}]\n{summary}")
    
    formatted_summary = "\n\n".join(summary_parts)
    
    return f"""
ì‹œê°„ ì œí•œìœ¼ë¡œ ì¸í•´ í˜„ì¬ê¹Œì§€ ì°¾ì€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ë“œë¦½ë‹ˆë‹¤.
ë” ìì„¸í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ ì¶”ê°€ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.

{formatted_summary}
"""

# ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í•¨ìˆ˜
async def process_user_input(user_input, history):
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        st.session_state.event_loop = loop
        
        # í˜„ì¬ ì‹œê°„ê³¼ ë§ˆì§€ë§‰ ì§ˆë¬¸ ì‹œê°„ì˜ ì°¨ì´ ê³„ì‚°
        current_time = time.time()
        if st.session_state.last_question_time:
            time_diff = current_time - st.session_state.last_question_time
            st.session_state.is_followup_question = time_diff < 30
        
        # 1ì°¨ ì§ˆë¬¸ì¸ ê²½ìš° ë¹ ë¥¸ ì‘ë‹µ ìƒì„±
        if not st.session_state.is_followup_question:
            try:
                async with asyncio.timeout(INITIAL_RESPONSE_TIMEOUT):
                    # ë¹ ë¥¸ ì´ˆê¸° ì‘ë‹µ ìƒì„±
                    answer = get_quick_response(user_input)
                    st.session_state.last_question_time = current_time
                    return answer
            except asyncio.TimeoutError:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
        
        # í›„ì† ì§ˆë¬¸ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        relevant_categories = analyze_question_categories(user_input)
        partial_responses = []
        found_relevant_answer = False
        
        try:
            async with asyncio.timeout(FOLLOWUP_RESPONSE_TIMEOUT):
                # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ì¹´í…Œê³ ë¦¬ë¶€í„° ì²˜ë¦¬
                for category in sorted(relevant_categories, key=lambda x: CATEGORY_PRIORITY[x]):
                    if found_relevant_answer:
                        break
                        
                    async for response in stream_agent_responses(user_input, history, category):
                        partial_responses.append(response)
                        if is_response_relevant(response[1], user_input):
                            found_relevant_answer = True
                            break
                
                if not found_relevant_answer:
                    remaining_categories = set(LAW_CATEGORIES.keys()) - set(relevant_categories)
                    for category in sorted(remaining_categories, key=lambda x: CATEGORY_PRIORITY[x]):
                        async for response in stream_agent_responses(user_input, history, category):
                            partial_responses.append(response)
                
                answer = get_head_agent_response(partial_responses, user_input, history)
                
        except asyncio.TimeoutError:
            if partial_responses:
                answer = generate_quick_summary(partial_responses, user_input)
            else:
                answer = get_quick_response(user_input)
        
        st.session_state.last_question_time = current_time
        return answer
        
    finally:
        loop.close()
        st.session_state.event_loop = None

def analyze_question_categories(question):
    """
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ì¹´í…Œê³ ë¦¬ë¥¼ ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ë°˜í™˜
    """
    relevant_categories = []
    question_lower = question.lower()
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(keyword in question_lower for keyword in keywords):
            relevant_categories.append(category)
    
    # ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ìš°ì„ ìˆœìœ„ ë°˜í™˜
    if not relevant_categories:
        return list(CATEGORY_PRIORITY.keys())
    
    return relevant_categories

def is_response_relevant(response, question):
    """
    ì‘ë‹µì˜ ê´€ë ¨ì„±ì„ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜
    """
    # ì‘ë‹µì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê´€ë ¨ì„±ì´ ë‚®ë‹¤ê³  íŒë‹¨
    if len(response) < 100:
        return False
        
    # ì§ˆë¬¸ì˜ ì£¼ìš” í‚¤ì›Œë“œê°€ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    question_keywords = set(re.findall(r'\w+', question.lower()))
    response_text = response.lower()
    
    # í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ ê³„ì‚°
    matched_keywords = sum(1 for keyword in question_keywords 
                         if len(keyword) > 1 and keyword in response_text)
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ì´ 30% ì´ìƒì´ë©´ ê´€ë ¨ì„±ì´ ë†’ë‹¤ê³  íŒë‹¨
    return matched_keywords / len(question_keywords) >= 0.3 if question_keywords else False

async def stream_agent_responses(question, history, category):
    """
    íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œì— ëŒ€í•´ì„œë§Œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    for law_name, pdf_path in LAW_CATEGORIES[category].items():
        try:
            response = await asyncio.wait_for(
                get_law_agent_response_async(law_name, question, history),
                timeout=1.0  # ê° ë¬¸ì„œë‹¹ 1ì´ˆë¡œ ì œí•œ
            )
            yield response
        except asyncio.TimeoutError:
            continue
        except Exception as e:
            print(f"Error processing {law_name}: {str(e)}")
            continue

def get_quick_response(question):
    """
    ë¹ ë¥¸ ì´ˆê¸° ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    model = get_model()
    prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ 10ì´ˆ ì´ë‚´ë¡œ í•µì‹¬ì ì¸ ë‹µë³€ë§Œ ê°„ë‹¨íˆ ì œê³µí•´ì£¼ì„¸ìš”.
í•„ìš”í•œ ê²½ìš° "ë” ìì„¸í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ ì¶”ê°€ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”"ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€ í˜•ì‹:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥)
2. ê´€ë ¨ í‚¤ì›Œë“œ
3. ì¶”ê°€ ì§ˆë¬¸ ìœ ë„
"""
    try:
        result = generate_content_with_retry(model, prompt)
        return result.text if result else "ì£„ì†¡í•©ë‹ˆë‹¤. ë¹ ë¥¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ë²•ë ¹ë³„ ì—ì´ì „íŠ¸ ì‘ë‹µ (async) ìˆ˜ì •
async def get_law_agent_response_async(law_name, question, history):
    if law_name not in st.session_state.embedding_data:
        text = st.session_state.law_data.get(law_name, "")
        vec, mat, chunks = create_embeddings_for_text(text)
        st.session_state.embedding_data[law_name] = (vec, mat, chunks)
    else:
        vec, mat, chunks = st.session_state.embedding_data[law_name]
    
    # ë¬¸ì„œ ìš”ì•½ ìš”ì²­ í™•ì¸
    if "ìš”ì•½" in question.lower() or "ì •ë¦¬" in question.lower():
        pdf_path = None
        for category in LAW_CATEGORIES.values():
            if law_name in category:
                pdf_path = category[law_name]
                break
        
        if pdf_path:
            summary = summarize_pdf_content(pdf_path)
            return law_name, summary
    
    context = search_relevant_chunks(question, vec, mat, chunks)
    
    supplier_info = None
    if any(keyword in question.lower() for keyword in ["ê³µê¸‰ì", "ìˆ˜ì¶œì", "ì œì¡°ì", "ì„¸ìœ¨", "ê´€ì„¸ìœ¨"]):
        supplier_info = get_dumping_rate("ì½”ë‹¥ê·¸ë˜í”½")
    
    prompt = f"""
ë‹¹ì‹ ì€ ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ë¤í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ëª¨ë“  ìë£Œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì•„ë˜ëŠ” ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìë£Œ ë‚´ìš©ì…ë‹ˆë‹¤:
{context}

{"ê³µê¸‰ì ì„¸ìœ¨ ì •ë³´:" + str(supplier_info) if supplier_info else ""}

ì´ì „ ëŒ€í™”:
{history}

ì§ˆë¬¸: {question}

# ì‘ë‹µ ì§€ì¹¨
1. ë‹µë³€ êµ¬ì¡°:
   - í•µì‹¬ ë‚´ìš© ìš”ì•½ (2-3ë¬¸ì¥)
   - ë²•ì  ê·¼ê±° (ê´€ë ¨ ì¡°í•­ êµ¬ì²´ì  ì¸ìš©)
   - ì„¸ë¶€ ì„¤ëª… (ì‹¤ë¬´ì  ê´€ì  í¬í•¨)
   - ì˜ˆì™¸ì‚¬í•­ ë˜ëŠ” ì£¼ì˜ì‚¬í•­

2. í˜•ì‹:
   - ì¤‘ìš” ìˆ˜ì¹˜, ê¸°í•œ, ì¡°í•­ì€ êµµê²Œ ê°•ì¡°
   - ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…
   - ë‹¨ê³„ì  ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš° ë²ˆí˜¸ ë§¤ê¸°ê¸°
   - ê´€ë ¨ ì¡°í•­ì€ ì •í™•í•œ ì¶œì²˜ì™€ í•¨ê»˜ ì¸ìš©

3. ë‚´ìš©:
   - í•´ë‹¹ ë²•ë ¹ì˜ íŠ¹ìˆ˜ì„± ë°˜ì˜
   - ë‹¤ë¥¸ ë²•ë ¹ê³¼ì˜ ê´€ê³„ ì„¤ëª…
   - ì‹¤ë¬´ì  ì ìš© ë°©ë²• ì œì‹œ
   - ìµœì‹  ê°œì •ì‚¬í•­ ë°˜ì˜

4. ì‹¤ìš©ì„±:
   - ì‹¤ì œ ì‚¬ë¡€ ì—°ê³„ (ê°€ëŠ¥í•œ ê²½ìš°)
   - ì‹¤ë¬´ì ê´€ì ì˜ í•´ì„ ì¶”ê°€
   - êµ¬ì²´ì ì¸ ì ìš© ë°©ë²• ì„¤ëª…
   - ê´€ë ¨ íŒë¡€ë‚˜ ê²°ì •ë¡€ ì¸ìš©

5. ì „ë¬¸ì„±:
   - êµ­ì œë¬´ì—­ë²•ì  ë§¥ë½ ê³ ë ¤
   - ì‚°ì—… íŠ¹ì„± ë°˜ì˜
   - WTO í˜‘ì • ë“± êµ­ì œê·œë²”ê³¼ì˜ ê´€ê³„
   - ìœ ì‚¬ ì‚¬ë¡€ë‚˜ ë¹„êµë²•ì  ë¶„ì„
"""
    model = get_model()
    result = generate_content_with_retry(model, prompt)
    return law_name, result.text if result else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# í—¤ë“œ ì—ì´ì „íŠ¸ í†µí•© ë‹µë³€ ìˆ˜ì •
def get_head_agent_response(responses, question, history):
    combined = "\n\n".join([f"=== {n} ê´€ë ¨ ì •ë³´ ===\n{r}" for n, r in responses])
    prompt = f"""
ë‹¹ì‹ ì€ ì¤‘êµ­ì‚° ì¸ì‡„ì œíŒìš© í‰ë©´ëª¨ì–‘ ì‚¬ì§„í”Œë ˆì´íŠ¸ ë¤í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ìë£Œì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ í¬ê´„ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

{combined}

ì´ì „ ëŒ€í™”:
{history}

ì§ˆë¬¸: {question}

# ì‘ë‹µ ì§€ì¹¨
1. ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:
   - í•µì‹¬ ë‹µë³€ (2-3ë¬¸ì¥ìœ¼ë¡œ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ë¨¼ì € ë‹µë³€)
   - ìƒì„¸ ì„¤ëª… (ê´€ë ¨ ë²•ë ¹, ê·œì •, íŒë¡€ ë“±ì„ ì¸ìš©í•˜ì—¬ êµ¬ì²´ì  ì„¤ëª…)
   - ê´€ë ¨ ì •ë³´ (ì¶”ê°€ë¡œ ì•Œì•„ë‘ë©´ ì¢‹ì€ ì •ë³´ë‚˜ ì—°ê´€ëœ ë‚´ìš©)
   - ì°¸ê³  ì‚¬í•­ (ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì˜ˆì™¸ì‚¬í•­ì´ ìˆë‹¤ë©´ ëª…ì‹œ)

2. í˜•ì‹ ìš”êµ¬ì‚¬í•­:
   - ê° ì„¹ì…˜ì€ ëª…í™•í•œ ì œëª©ìœ¼ë¡œ êµ¬ë¶„
   - ì¤‘ìš”í•œ ìˆ˜ì¹˜ë‚˜ ë‚ ì§œëŠ” êµµì€ ê¸€ì”¨ë¡œ ê°•ì¡°
   - ë²•ë ¹ ì¸ìš© ì‹œ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œ
   - ëª©ë¡í™”ê°€ ê°€ëŠ¥í•œ ë‚´ìš©ì€ ë²ˆí˜¸ë‚˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ êµ¬ë¶„

3. ë‚´ìš© ìš”êµ¬ì‚¬í•­:
   - ëª¨ë“  ì£¼ì¥ì— ëŒ€í•œ ê·¼ê±° ì œì‹œ
   - ì‹¤ë¬´ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì •ë³´ ê°•ì¡°
   - ìµœì‹  ê°œì •ì‚¬í•­ì´ë‚˜ ë³€ê²½ì  ë°˜ì˜
   - ì‹¤ì œ ì‚¬ë¡€ë‚˜ ì˜ˆì‹œ í¬í•¨ (ê°€ëŠ¥í•œ ê²½ìš°)

4. ì „ë¬¸ì„± ìš”êµ¬ì‚¬í•­:
   - ì „ë¬¸ ìš©ì–´ëŠ” í’€ì–´ì„œ ì„¤ëª…
   - ë²•ì  í•´ì„ì´ í•„ìš”í•œ ê²½ìš° ê´€ë ¨ ë²•ë ¹ í•¨ê»˜ ì œì‹œ
   - ì‚°ì—… í˜„ì¥ì˜ ì‹¤ë¬´ì  ê´€ì  ë°˜ì˜
   - êµ­ì œë¬´ì—­ë²•ì  ë§¥ë½ ê³ ë ¤

5. ê°€ë…ì„± ìš”êµ¬ì‚¬í•­:
   - ë‹¨ë½ì„ ì ì ˆíˆ êµ¬ë¶„í•˜ì—¬ ê°€ë…ì„± í™•ë³´
   - ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…
   - í•„ìš”ì‹œ í‘œë‚˜ êµ¬ë¶„ì„  ì‚¬ìš©
   - ì „ì²´ì ì¸ ë¬¸ë§¥ì˜ íë¦„ ìœ ì§€
"""
    model = get_model()
    result = generate_content_with_retry(model, prompt)
    return result.text if result else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

# ëŒ€í™” ê¸°ë¡ ë Œë”ë§
for msg in st.session_state.chat_history:
    with st.chat_message(msg['role']):
        st.markdown(msg['content'])

# ëª¨ë“  ì—ì´ì „íŠ¸ ë³‘ë ¬ ì‹¤í–‰
async def gather_agent_responses(question, history):
    tasks = []
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ íƒœìŠ¤í¬ ìƒì„±
    for category in LAW_CATEGORIES.values():
        for law_name, pdf_path in category.items():
            tasks.append(get_law_agent_response_async(law_name, question, history))
    return await asyncio.gather(*tasks)

# ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ ë¶€ë¶„ ìˆ˜ì •
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="main_chat_input"):
    # ìƒˆë¡œìš´ ì§ˆë¬¸ ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    
    # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ë‹µë³€ ìƒì„±
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            # ëª¨ë“  ë¬¸ì„œë¥¼ í•œë²ˆì— ë¡œë“œ
            if not st.session_state.law_data:
                st.session_state.law_data = load_law_data()
            
            history = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.chat_history])
            
            # ë¹„ë™ê¸° ì²˜ë¦¬
            answer = asyncio.run(process_user_input(user_input, history))
            
            if answer:
                # ë‹µë³€ì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
                st.session_state.chat_history.append({"role": "assistant", "content": answer})
                
                # ì±„íŒ… ê¸°ë¡ ì—…ë°ì´íŠ¸
                with st.chat_message("assistant"):
                    st.markdown(answer)
            else:
                st.error("ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.session_state.chat_history.pop()  # ì‹¤íŒ¨í•œ ì§ˆë¬¸ ì œê±°
